{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using DataFrames\n",
    "    using LazyJSON\n",
    "    using GLM \n",
    "    using MLJ \n",
    "    using MLJBase\n",
    "    using CSV\n",
    "    using Serialization\n",
    "    using MLJScientificTypes\n",
    "    using CategoricalArrays\n",
    "    using MLJLinearModels\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES \n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "OUTPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"outputs\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.ser\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.ser\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "PREDICTIONS_DIR = joinpath(OUTPUT_DIR, \"predictions\")\n",
    "PREDICTIONS_FILE = joinpath(PREDICTIONS_DIR, \"predictions.csv\")\n",
    "TARGET_LEVELS = joinpath(MODEL_ARTIFACTS_PATH, \"target_levels.ser\")\n",
    "TARGET_MAPPING = joinpath(MODEL_ARTIFACTS_PATH, \"target_mapping.ser\")\n",
    "\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"M\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]\n",
    "target_classes = schema[\"target\"][\"classes\"]\n",
    "\n",
    "if length(target_classes) == 2\n",
    "    negative_class = target_classes[1]\n",
    "    positive_class = target_classes[2]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>114 rows × 31 columns (omitted printing of 25 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>87930</td><td>12.47</td><td>18.6</td><td>81.09</td><td>481.9</td><td>0.09965</td></tr><tr><th>2</th><td>859575</td><td>18.94</td><td>21.31</td><td>123.6</td><td>1130.0</td><td>0.09009</td></tr><tr><th>3</th><td>8670</td><td>15.46</td><td>19.48</td><td>101.7</td><td>748.9</td><td>0.1092</td></tr><tr><th>4</th><td>907915</td><td>12.4</td><td>17.68</td><td>81.47</td><td>467.8</td><td>0.1054</td></tr><tr><th>5</th><td>921385</td><td>11.54</td><td>14.44</td><td>74.65</td><td>402.9</td><td>0.09984</td></tr><tr><th>6</th><td>927241</td><td>20.6</td><td>29.33</td><td>140.1</td><td>1265.0</td><td>0.1178</td></tr><tr><th>7</th><td>9012000</td><td>22.01</td><td>21.9</td><td>147.2</td><td>1482.0</td><td>0.1063</td></tr><tr><th>8</th><td>853201</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>9</th><td>8611161</td><td>13.34</td><td>15.86</td><td>86.49</td><td>520.0</td><td>0.1078</td></tr><tr><th>10</th><td>911673</td><td>13.9</td><td>16.62</td><td>88.97</td><td>599.4</td><td>0.06828</td></tr><tr><th>11</th><td>9112367</td><td>13.21</td><td>25.25</td><td>84.1</td><td>537.9</td><td>0.08791</td></tr><tr><th>12</th><td>8712729</td><td>16.78</td><td>18.8</td><td>109.3</td><td>886.3</td><td>0.08865</td></tr><tr><th>13</th><td>8712291</td><td>14.97</td><td>19.76</td><td>95.5</td><td>690.2</td><td>0.08421</td></tr><tr><th>14</th><td>895633</td><td>16.26</td><td>21.88</td><td>107.5</td><td>826.8</td><td>0.1165</td></tr><tr><th>15</th><td>91813702</td><td>12.34</td><td>12.27</td><td>78.94</td><td>468.5</td><td>0.09003</td></tr><tr><th>16</th><td>8611792</td><td>19.1</td><td>26.29</td><td>129.1</td><td>1132.0</td><td>0.1215</td></tr><tr><th>17</th><td>915664</td><td>14.81</td><td>14.7</td><td>94.66</td><td>680.7</td><td>0.08472</td></tr><tr><th>18</th><td>924964</td><td>10.16</td><td>19.59</td><td>64.73</td><td>311.7</td><td>0.1003</td></tr><tr><th>19</th><td>862722</td><td>6.981</td><td>13.43</td><td>43.79</td><td>143.5</td><td>0.117</td></tr><tr><th>20</th><td>919555</td><td>20.55</td><td>20.86</td><td>137.8</td><td>1308.0</td><td>0.1046</td></tr><tr><th>21</th><td>859983</td><td>13.8</td><td>15.79</td><td>90.43</td><td>584.1</td><td>0.1007</td></tr><tr><th>22</th><td>903554</td><td>12.1</td><td>17.72</td><td>78.07</td><td>446.2</td><td>0.1029</td></tr><tr><th>23</th><td>903516</td><td>21.61</td><td>22.28</td><td>144.4</td><td>1407.0</td><td>0.1167</td></tr><tr><th>24</th><td>907367</td><td>10.03</td><td>21.28</td><td>63.19</td><td>307.3</td><td>0.08117</td></tr><tr><th>25</th><td>893061</td><td>11.6</td><td>24.49</td><td>74.23</td><td>417.2</td><td>0.07474</td></tr><tr><th>26</th><td>8610629</td><td>13.53</td><td>10.94</td><td>87.91</td><td>559.2</td><td>0.1291</td></tr><tr><th>27</th><td>902727</td><td>13.28</td><td>13.72</td><td>85.79</td><td>541.8</td><td>0.08363</td></tr><tr><th>28</th><td>924934</td><td>10.29</td><td>27.61</td><td>65.67</td><td>321.4</td><td>0.0903</td></tr><tr><th>29</th><td>9010598</td><td>12.76</td><td>18.84</td><td>81.87</td><td>496.6</td><td>0.09676</td></tr><tr><th>30</th><td>859717</td><td>17.2</td><td>24.52</td><td>114.2</td><td>929.4</td><td>0.1071</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& id & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 87930 & 12.47 & 18.6 & 81.09 & 481.9 & 0.09965 & $\\dots$ \\\\\n",
       "\t2 & 859575 & 18.94 & 21.31 & 123.6 & 1130.0 & 0.09009 & $\\dots$ \\\\\n",
       "\t3 & 8670 & 15.46 & 19.48 & 101.7 & 748.9 & 0.1092 & $\\dots$ \\\\\n",
       "\t4 & 907915 & 12.4 & 17.68 & 81.47 & 467.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t5 & 921385 & 11.54 & 14.44 & 74.65 & 402.9 & 0.09984 & $\\dots$ \\\\\n",
       "\t6 & 927241 & 20.6 & 29.33 & 140.1 & 1265.0 & 0.1178 & $\\dots$ \\\\\n",
       "\t7 & 9012000 & 22.01 & 21.9 & 147.2 & 1482.0 & 0.1063 & $\\dots$ \\\\\n",
       "\t8 & 853201 & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t9 & 8611161 & 13.34 & 15.86 & 86.49 & 520.0 & 0.1078 & $\\dots$ \\\\\n",
       "\t10 & 911673 & 13.9 & 16.62 & 88.97 & 599.4 & 0.06828 & $\\dots$ \\\\\n",
       "\t11 & 9112367 & 13.21 & 25.25 & 84.1 & 537.9 & 0.08791 & $\\dots$ \\\\\n",
       "\t12 & 8712729 & 16.78 & 18.8 & 109.3 & 886.3 & 0.08865 & $\\dots$ \\\\\n",
       "\t13 & 8712291 & 14.97 & 19.76 & 95.5 & 690.2 & 0.08421 & $\\dots$ \\\\\n",
       "\t14 & 895633 & 16.26 & 21.88 & 107.5 & 826.8 & 0.1165 & $\\dots$ \\\\\n",
       "\t15 & 91813702 & 12.34 & 12.27 & 78.94 & 468.5 & 0.09003 & $\\dots$ \\\\\n",
       "\t16 & 8611792 & 19.1 & 26.29 & 129.1 & 1132.0 & 0.1215 & $\\dots$ \\\\\n",
       "\t17 & 915664 & 14.81 & 14.7 & 94.66 & 680.7 & 0.08472 & $\\dots$ \\\\\n",
       "\t18 & 924964 & 10.16 & 19.59 & 64.73 & 311.7 & 0.1003 & $\\dots$ \\\\\n",
       "\t19 & 862722 & 6.981 & 13.43 & 43.79 & 143.5 & 0.117 & $\\dots$ \\\\\n",
       "\t20 & 919555 & 20.55 & 20.86 & 137.8 & 1308.0 & 0.1046 & $\\dots$ \\\\\n",
       "\t21 & 859983 & 13.8 & 15.79 & 90.43 & 584.1 & 0.1007 & $\\dots$ \\\\\n",
       "\t22 & 903554 & 12.1 & 17.72 & 78.07 & 446.2 & 0.1029 & $\\dots$ \\\\\n",
       "\t23 & 903516 & 21.61 & 22.28 & 144.4 & 1407.0 & 0.1167 & $\\dots$ \\\\\n",
       "\t24 & 907367 & 10.03 & 21.28 & 63.19 & 307.3 & 0.08117 & $\\dots$ \\\\\n",
       "\t25 & 893061 & 11.6 & 24.49 & 74.23 & 417.2 & 0.07474 & $\\dots$ \\\\\n",
       "\t26 & 8610629 & 13.53 & 10.94 & 87.91 & 559.2 & 0.1291 & $\\dots$ \\\\\n",
       "\t27 & 902727 & 13.28 & 13.72 & 85.79 & 541.8 & 0.08363 & $\\dots$ \\\\\n",
       "\t28 & 924934 & 10.29 & 27.61 & 65.67 & 321.4 & 0.0903 & $\\dots$ \\\\\n",
       "\t29 & 9010598 & 12.76 & 18.84 & 81.87 & 496.6 & 0.09676 & $\\dots$ \\\\\n",
       "\t30 & 859717 & 17.2 & 24.52 & 114.2 & 929.4 & 0.1071 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m114×31 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id       \u001b[0m\u001b[1m radius_mean \u001b[0m\u001b[1m texture_mean \u001b[0m\u001b[1m perimeter_mean \u001b[0m\u001b[1m area_mean \u001b[0m\u001b[1m smoothn\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │    87930       12.47          18.6            81.09      481.9          ⋯\n",
       "   2 │   859575       18.94          21.31          123.6      1130.0\n",
       "   3 │     8670       15.46          19.48          101.7       748.9\n",
       "   4 │   907915       12.4           17.68           81.47      467.8\n",
       "   5 │   921385       11.54          14.44           74.65      402.9          ⋯\n",
       "   6 │   927241       20.6           29.33          140.1      1265.0\n",
       "   7 │  9012000       22.01          21.9           147.2      1482.0\n",
       "   8 │   853201       17.57          15.05          115.0       955.1\n",
       "   9 │  8611161       13.34          15.86           86.49      520.0          ⋯\n",
       "  10 │   911673       13.9           16.62           88.97      599.4\n",
       "  11 │  9112367       13.21          25.25           84.1       537.9\n",
       "  ⋮  │    ⋮           ⋮            ⋮              ⋮             ⋮              ⋱\n",
       " 105 │   923465       10.82          24.21           68.89      361.6\n",
       " 106 │    91858       11.75          17.56           75.89      422.9          ⋯\n",
       " 107 │  8712064       12.34          22.22           79.85      464.5\n",
       " 108 │   915143       23.09          19.83          152.1      1682.0\n",
       " 109 │ 86973702       14.44          15.18           93.97      640.1\n",
       " 110 │   913102       14.64          16.85           94.21      666.0          ⋯\n",
       " 111 │  8610404       16.07          19.65          104.1       817.7\n",
       " 112 │   884689       11.52          14.93           73.87      406.3\n",
       " 113 │   883270       14.22          27.85           92.55      623.9\n",
       " 114 │ 88995002       20.73          31.12          135.7      1419.0          ⋯\n",
       "\u001b[36m                                                  26 columns and 93 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = filter(x -> occursin(\".csv\", x), readdir(TEST_DIR))[1]\n",
    "file_path = joinpath(TEST_DIR, file_name)\n",
    "df = DataFrame(CSV.File(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Note that when we work with testing data, we have to impute using the same values learned during training. This is to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>114 rows × 30 columns (omitted printing of 24 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th><th>compactness_mean</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>12.47</td><td>18.6</td><td>81.09</td><td>481.9</td><td>0.09965</td><td>0.1058</td></tr><tr><th>2</th><td>18.94</td><td>21.31</td><td>123.6</td><td>1130.0</td><td>0.09009</td><td>0.1029</td></tr><tr><th>3</th><td>15.46</td><td>19.48</td><td>101.7</td><td>748.9</td><td>0.1092</td><td>0.1223</td></tr><tr><th>4</th><td>12.4</td><td>17.68</td><td>81.47</td><td>467.8</td><td>0.1054</td><td>0.1316</td></tr><tr><th>5</th><td>11.54</td><td>14.44</td><td>74.65</td><td>402.9</td><td>0.09984</td><td>0.112</td></tr><tr><th>6</th><td>20.6</td><td>29.33</td><td>140.1</td><td>1265.0</td><td>0.1178</td><td>0.277</td></tr><tr><th>7</th><td>22.01</td><td>21.9</td><td>147.2</td><td>1482.0</td><td>0.1063</td><td>0.1954</td></tr><tr><th>8</th><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td><td>0.1157</td></tr><tr><th>9</th><td>13.34</td><td>15.86</td><td>86.49</td><td>520.0</td><td>0.1078</td><td>0.1535</td></tr><tr><th>10</th><td>13.9</td><td>16.62</td><td>88.97</td><td>599.4</td><td>0.06828</td><td>0.05319</td></tr><tr><th>11</th><td>13.21</td><td>25.25</td><td>84.1</td><td>537.9</td><td>0.08791</td><td>0.05205</td></tr><tr><th>12</th><td>16.78</td><td>18.8</td><td>109.3</td><td>886.3</td><td>0.08865</td><td>0.09182</td></tr><tr><th>13</th><td>14.97</td><td>19.76</td><td>95.5</td><td>690.2</td><td>0.08421</td><td>0.05352</td></tr><tr><th>14</th><td>16.26</td><td>21.88</td><td>107.5</td><td>826.8</td><td>0.1165</td><td>0.1283</td></tr><tr><th>15</th><td>12.34</td><td>12.27</td><td>78.94</td><td>468.5</td><td>0.09003</td><td>0.06307</td></tr><tr><th>16</th><td>19.1</td><td>26.29</td><td>129.1</td><td>1132.0</td><td>0.1215</td><td>0.1791</td></tr><tr><th>17</th><td>14.81</td><td>14.7</td><td>94.66</td><td>680.7</td><td>0.08472</td><td>0.05016</td></tr><tr><th>18</th><td>10.16</td><td>19.59</td><td>64.73</td><td>311.7</td><td>0.1003</td><td>0.07504</td></tr><tr><th>19</th><td>6.981</td><td>13.43</td><td>43.79</td><td>143.5</td><td>0.117</td><td>0.07568</td></tr><tr><th>20</th><td>20.55</td><td>20.86</td><td>137.8</td><td>1308.0</td><td>0.1046</td><td>0.1739</td></tr><tr><th>21</th><td>13.8</td><td>15.79</td><td>90.43</td><td>584.1</td><td>0.1007</td><td>0.128</td></tr><tr><th>22</th><td>12.1</td><td>17.72</td><td>78.07</td><td>446.2</td><td>0.1029</td><td>0.09758</td></tr><tr><th>23</th><td>21.61</td><td>22.28</td><td>144.4</td><td>1407.0</td><td>0.1167</td><td>0.2087</td></tr><tr><th>24</th><td>10.03</td><td>21.28</td><td>63.19</td><td>307.3</td><td>0.08117</td><td>0.03912</td></tr><tr><th>25</th><td>11.6</td><td>24.49</td><td>74.23</td><td>417.2</td><td>0.07474</td><td>0.05688</td></tr><tr><th>26</th><td>13.53</td><td>10.94</td><td>87.91</td><td>559.2</td><td>0.1291</td><td>0.1047</td></tr><tr><th>27</th><td>13.28</td><td>13.72</td><td>85.79</td><td>541.8</td><td>0.08363</td><td>0.08575</td></tr><tr><th>28</th><td>10.29</td><td>27.61</td><td>65.67</td><td>321.4</td><td>0.0903</td><td>0.07658</td></tr><tr><th>29</th><td>12.76</td><td>18.84</td><td>81.87</td><td>496.6</td><td>0.09676</td><td>0.07952</td></tr><tr><th>30</th><td>17.2</td><td>24.52</td><td>114.2</td><td>929.4</td><td>0.1071</td><td>0.183</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & compactness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 12.47 & 18.6 & 81.09 & 481.9 & 0.09965 & 0.1058 & $\\dots$ \\\\\n",
       "\t2 & 18.94 & 21.31 & 123.6 & 1130.0 & 0.09009 & 0.1029 & $\\dots$ \\\\\n",
       "\t3 & 15.46 & 19.48 & 101.7 & 748.9 & 0.1092 & 0.1223 & $\\dots$ \\\\\n",
       "\t4 & 12.4 & 17.68 & 81.47 & 467.8 & 0.1054 & 0.1316 & $\\dots$ \\\\\n",
       "\t5 & 11.54 & 14.44 & 74.65 & 402.9 & 0.09984 & 0.112 & $\\dots$ \\\\\n",
       "\t6 & 20.6 & 29.33 & 140.1 & 1265.0 & 0.1178 & 0.277 & $\\dots$ \\\\\n",
       "\t7 & 22.01 & 21.9 & 147.2 & 1482.0 & 0.1063 & 0.1954 & $\\dots$ \\\\\n",
       "\t8 & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & 0.1157 & $\\dots$ \\\\\n",
       "\t9 & 13.34 & 15.86 & 86.49 & 520.0 & 0.1078 & 0.1535 & $\\dots$ \\\\\n",
       "\t10 & 13.9 & 16.62 & 88.97 & 599.4 & 0.06828 & 0.05319 & $\\dots$ \\\\\n",
       "\t11 & 13.21 & 25.25 & 84.1 & 537.9 & 0.08791 & 0.05205 & $\\dots$ \\\\\n",
       "\t12 & 16.78 & 18.8 & 109.3 & 886.3 & 0.08865 & 0.09182 & $\\dots$ \\\\\n",
       "\t13 & 14.97 & 19.76 & 95.5 & 690.2 & 0.08421 & 0.05352 & $\\dots$ \\\\\n",
       "\t14 & 16.26 & 21.88 & 107.5 & 826.8 & 0.1165 & 0.1283 & $\\dots$ \\\\\n",
       "\t15 & 12.34 & 12.27 & 78.94 & 468.5 & 0.09003 & 0.06307 & $\\dots$ \\\\\n",
       "\t16 & 19.1 & 26.29 & 129.1 & 1132.0 & 0.1215 & 0.1791 & $\\dots$ \\\\\n",
       "\t17 & 14.81 & 14.7 & 94.66 & 680.7 & 0.08472 & 0.05016 & $\\dots$ \\\\\n",
       "\t18 & 10.16 & 19.59 & 64.73 & 311.7 & 0.1003 & 0.07504 & $\\dots$ \\\\\n",
       "\t19 & 6.981 & 13.43 & 43.79 & 143.5 & 0.117 & 0.07568 & $\\dots$ \\\\\n",
       "\t20 & 20.55 & 20.86 & 137.8 & 1308.0 & 0.1046 & 0.1739 & $\\dots$ \\\\\n",
       "\t21 & 13.8 & 15.79 & 90.43 & 584.1 & 0.1007 & 0.128 & $\\dots$ \\\\\n",
       "\t22 & 12.1 & 17.72 & 78.07 & 446.2 & 0.1029 & 0.09758 & $\\dots$ \\\\\n",
       "\t23 & 21.61 & 22.28 & 144.4 & 1407.0 & 0.1167 & 0.2087 & $\\dots$ \\\\\n",
       "\t24 & 10.03 & 21.28 & 63.19 & 307.3 & 0.08117 & 0.03912 & $\\dots$ \\\\\n",
       "\t25 & 11.6 & 24.49 & 74.23 & 417.2 & 0.07474 & 0.05688 & $\\dots$ \\\\\n",
       "\t26 & 13.53 & 10.94 & 87.91 & 559.2 & 0.1291 & 0.1047 & $\\dots$ \\\\\n",
       "\t27 & 13.28 & 13.72 & 85.79 & 541.8 & 0.08363 & 0.08575 & $\\dots$ \\\\\n",
       "\t28 & 10.29 & 27.61 & 65.67 & 321.4 & 0.0903 & 0.07658 & $\\dots$ \\\\\n",
       "\t29 & 12.76 & 18.84 & 81.87 & 496.6 & 0.09676 & 0.07952 & $\\dots$ \\\\\n",
       "\t30 & 17.2 & 24.52 & 114.2 & 929.4 & 0.1071 & 0.183 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m114×30 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m radius_mean \u001b[0m\u001b[1m texture_mean \u001b[0m\u001b[1m perimeter_mean \u001b[0m\u001b[1m area_mean \u001b[0m\u001b[1m smoothness_mean \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64         \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │      12.47          18.6            81.09      481.9          0.09965   ⋯\n",
       "   2 │      18.94          21.31          123.6      1130.0          0.09009\n",
       "   3 │      15.46          19.48          101.7       748.9          0.1092\n",
       "   4 │      12.4           17.68           81.47      467.8          0.1054\n",
       "   5 │      11.54          14.44           74.65      402.9          0.09984   ⋯\n",
       "   6 │      20.6           29.33          140.1      1265.0          0.1178\n",
       "   7 │      22.01          21.9           147.2      1482.0          0.1063\n",
       "   8 │      17.57          15.05          115.0       955.1          0.09847\n",
       "   9 │      13.34          15.86           86.49      520.0          0.1078    ⋯\n",
       "  10 │      13.9           16.62           88.97      599.4          0.06828\n",
       "  11 │      13.21          25.25           84.1       537.9          0.08791\n",
       "  ⋮  │      ⋮            ⋮              ⋮             ⋮             ⋮          ⋱\n",
       " 105 │      10.82          24.21           68.89      361.6          0.08192\n",
       " 106 │      11.75          17.56           75.89      422.9          0.1073    ⋯\n",
       " 107 │      12.34          22.22           79.85      464.5          0.1012\n",
       " 108 │      23.09          19.83          152.1      1682.0          0.09342\n",
       " 109 │      14.44          15.18           93.97      640.1          0.0997\n",
       " 110 │      14.64          16.85           94.21      666.0          0.08641   ⋯\n",
       " 111 │      16.07          19.65          104.1       817.7          0.09168\n",
       " 112 │      11.52          14.93           73.87      406.3          0.1013\n",
       " 113 │      14.22          27.85           92.55      623.9          0.08223\n",
       " 114 │      20.73          31.12          135.7      1419.0          0.09469   ⋯\n",
       "\u001b[36m                                                  25 columns and 93 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_values = open(deserialize, IMPUTATION_FILE)\n",
    "for column in nullable_features\n",
    "    df[!, Symbol(column)] .= coalesce.(df[!, Symbol(column)], get(imputation_values, string(column), missing))\n",
    "end\n",
    "\n",
    "# Saving the id column in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "\n",
    "# Dropping the id and target from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "We encode the data using the same encoder that we saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_top_categories = open(deserialize, TOP_CATEGORIES)\n",
    "\n",
    "# Function to one-hot encode only the top 10 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = \"$(feature)_binary\"\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = \"$(feature)_$(cat)\"\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "one_hot_top_categories!(df, loaded_top_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "Using the model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/moo/Desktop/Upwork/rt-ML/Julia/Julia-Logistic-Regression-Template/model_inputs_outputs/outputs/predictions/predictions.csv\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = open(deserialize, PREDICTOR_FILE_PATH)\n",
    "target_levels = open(deserialize, TARGET_LEVELS)\n",
    "\n",
    "\n",
    "# if length(target_classes) == 2\n",
    "#     loaded_mapping = open(deserialize, TARGET_MAPPING)\n",
    "#     log_odds_predictions = GLM.predict(model, convert(Matrix{Float64}, Matrix(df)))\n",
    "#     probabilities = 1 ./ (1 .+ exp.(-log_odds_predictions))\n",
    "#     threshold = 0.5\n",
    "#     integer_predictions = ifelse.(probabilities .> threshold, 1, 0)\n",
    "#     label_predictions = [loaded_mapping[pred] for pred in integer_predictions]\n",
    "#     label_predictions = target_levels[integer_predictions .+ 1]\n",
    "\n",
    "#     result_df = DataFrame()\n",
    "#     result_df[!, Symbol(negative_class)] = 1 .- probabilities\n",
    "#     result_df[!, Symbol(positive_class)] = probabilities\n",
    "#     result_df[!, id_feature] = ids\n",
    "#     result_df\n",
    "\n",
    "\n",
    "# else\n",
    "probabilities = MLJ.predict(model, df)\n",
    "\n",
    "# Number of classes\n",
    "n_classes = length(target_classes)\n",
    "\n",
    "# Extract probabilities for each class\n",
    "probs_matrix = hcat([pdf.(probabilities, level) for level in target_levels]...)\n",
    "\n",
    "# Create the DataFrame\n",
    "result_df = DataFrame(probs_matrix, Symbol.(target_levels))\n",
    "result_df[!, id_feature] = ids\n",
    "result_df\n",
    "\n",
    "# end\n",
    "CSV.write(PREDICTIONS_FILE, result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
